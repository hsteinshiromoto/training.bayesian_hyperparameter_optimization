{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.59s/trial, best loss: 0.04339285714285712]\n",
      "100%|██████████| 2/2 [00:03<00:00,  3.42s/trial, best loss: 0.04339285714285712]\n",
      "100%|██████████| 3/3 [00:06<00:00,  6.27s/trial, best loss: 0.04339285714285712]\n",
      "100%|██████████| 4/4 [03:04<00:00, 184.85s/trial, best loss: 0.02491071428571423]\n",
      "100%|██████████| 5/5 [04:59<00:00, 299.93s/trial, best loss: 0.02491071428571423]\n",
      "100%|██████████| 6/6 [05:03<00:00, 303.99s/trial, best loss: 0.02491071428571423]\n",
      "100%|██████████| 7/7 [05:01<00:00, 301.15s/trial, best loss: 0.02491071428571423]\n",
      "100%|██████████| 8/8 [02:50<00:00, 170.45s/trial, best loss: 0.02491071428571423]\n",
      "100%|██████████| 9/9 [05:00<00:00, 300.73s/trial, best loss: 0.02491071428571423]\n",
      " 90%|█████████ | 9/10 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.9/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:39:01] WARNING: ../src/learner.cc:573: \n",
      "Parameters: { \"scale_pos_weight\" } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[19:39:19] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "100%|██████████| 10/10 [05:01<00:00, 301.28s/trial, best loss: 0.02491071428571423]\n",
      "0.9732142857142857\n",
      "{'learner': ExtraTreesClassifier(criterion='entropy', max_features=0.4916021206314122,\n",
      "                     n_estimators=67, n_jobs=1, random_state=4, verbose=False), 'preprocs': (Normalizer(norm='l1'),), 'ex_preprocs': ()}\n"
     ]
    }
   ],
   "source": [
    "from hpsklearn import HyperoptEstimator, any_classifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "from hyperopt import tpe\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Download the data and split into training and test sets\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "test_size = int( 0.2 * len( y ) )\n",
    "indices = np.random.permutation(len(X))\n",
    "X_train = X[indices[:-test_size]]\n",
    "y_train = y[indices[:-test_size]]\n",
    "X_test  = X[indices[-test_size:]]\n",
    "y_test  = y[indices[-test_size:]]\n",
    "\n",
    "# Does not contain XGBoost\n",
    "\n",
    "estim = HyperoptEstimator(classifier=any_classifier('clf'),  \n",
    "                          algo=tpe.suggest, \n",
    "                          trial_timeout=300)\n",
    "\n",
    "estim.fit( X_train, y_train )\n",
    "\n",
    "print( estim.score( X_test, y_test ) )\n",
    "# <<show score here>>\n",
    "print( estim.best_model() )\n",
    "# <<show model here>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e\n",
    "\n",
    "https://hyperopt.github.io/hyperopt-sklearn/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}